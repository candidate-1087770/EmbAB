{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing and optmising Logistic Regression Model\n",
    "\n",
    "Split into 3 sections:   \n",
    "1) Logistic regression for direct binary classification   \n",
    "2) Logistic regression for indirect binary classification via mutliclass MIC classification   \n",
    "3) Logistic regression for indirect binary classification via mutliclass MIC classification with compressed MIC labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../notebooks')\n",
    "\n",
    "from Build_ML_df import *\n",
    "ML_df = df_for_ML().merged_structural()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ML_df[['BINARY_PHENOTYPE', 'MIC', 'Ligand0_Distance', 'Ca_Distance', \n",
    "            'cardiolipin_Distance', 'Depth', 'lipid_head_dis', 'lipid_tail_dis',\n",
    "            'dG_stability', 'd_volume', 'd_MW', 'd_hydropathy', 'Pi', 'MAPP', \n",
    "            'H', 'B', 'E', 'G', 'I', 'T', 'S', 'NaN']].copy()\n",
    "\n",
    "#convert foldx values to float\n",
    "df['dG_stability'] = df['dG_stability'].astype(float, errors='raise')\n",
    "#remove duplicates\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#Convert intermediate phenotypes to resistant\n",
    "List = []\n",
    "for i in df['BINARY_PHENOTYPE']:\n",
    "    if i == 'I':\n",
    "        List.append('R')\n",
    "    else:\n",
    "        List.append(i)\n",
    "        \n",
    "df['BINARY_PHENOTYPE'] = List\n",
    "\n",
    "#create numpy array with features for ML training\n",
    "data_array = df[df.columns[2:]].to_numpy()\n",
    "\n",
    "#create column withbinary phenotype\n",
    "List = []\n",
    "for i in df['BINARY_PHENOTYPE']:\n",
    "    if i == 'R':\n",
    "        List.append(0)\n",
    "    else:\n",
    "        List.append(1)\n",
    "df['BF'] = List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct binary classification\n",
    "\n",
    "#### Strategy for finding best performing model:\n",
    "1) Grid search for best performing preprocessing and parameters for *ACCURACY*   \n",
    "2) Grid search for best performing preprocessing and parameters for *AVERAGE PRECISION SCORE* \n",
    "3) Grid search for best performing preprocessing and parameters for *ROC AUC*  \n",
    "4) Generate precision-recall curve with best parameters for average precision   \n",
    "5) Generate ROC curve with best parameters for ROC AUC   \n",
    "6) Use curves to determine if precision and FPR can be improved   \n",
    "7) Generate confusiom matrix with best parameters for either average preicsion or ROC AUC (these tend to have the same best performing parameters)   \n",
    "8) Shift decision function thresholds to increase specificity and precision, and reduce very major error   \n",
    "9) This model is now fine tuned   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Grid Search for best performing preprocessing and parameters for Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "\n",
    "#create logistic regression pipeline with preprocessing \n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', LogisticRegression(max_iter=10000))])\n",
    "#create parameter grid with different preprocessing and classifier parameters\n",
    "param_grid = {'preprocessing':[StandardScaler(), MinMaxScaler(), RobustScaler(), None],\n",
    "              'classifier__C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "#split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Search with shuffled cross validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid, cv=Kfold_shuffle)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set average accuracy: ', \n",
    "       accuracy_score(y_test, grid_kfold_shuffle.predict(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Grid search for best performing preprocessing and parameters for AVERAGE PRECISION SCORE (different cross validation strategies trialed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with with shuffled cross validation\n",
    "#use decision function to calculate average_precision\n",
    "kfold_shuffle = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid, cv=kfold_shuffle, scoring='average_precision')\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)                  \n",
    "print ('grid_kfold_shuffle test set average precision: ', \n",
    "       average_precision_score(y_test, grid_kfold_shuffle.decision_function(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Grid search for best performing preprocessing and parameters for ROC AUC (different cross validation strategies trialed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with with shuffled kfold cross validation\n",
    "#use decision function to calculate AUC\n",
    "kfold_shuffle = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid, cv=kfold_shuffle, scoring='roc_auc')\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)                  \n",
    "print ('grid_kfold_shuffle test set AUC: ', \n",
    "       roc_auc_score(y_test, grid_kfold_shuffle.decision_function(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generate precision-recall curve with best parameters for average precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best preprocessing (standard scaler) and parameters (c=100) for av. precision\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "pipe = Pipeline([('preprocessing', RobustScaler()), \n",
    "                 ('classifier', LogisticRegression(C=100, max_iter=10000))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "pipe.fit(X_train, y_train)\n",
    "predict = (pipe.decision_function(X_test))\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, predict)\n",
    "\n",
    "plt.plot(precision, recall, label='LogReg')\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(precision[close_zero], recall[close_zero], '^', c='k', \n",
    "          markersize=10, label='threshold zero ', fillstyle='none', mew=2)\n",
    "plt.xlabel('Precision')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Precision-recall curve for logistic regression (StandardScaler(), C=100)')\n",
    "plt.savefig('Precision_recall_LR_binary.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Generate ROC curve with best parameters for ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best preprocessing (standard scaler) and parameters (c=100) for av. precision\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "pipe = Pipeline([('preprocessing', MinMaxScaler()), \n",
    "                 ('classifier', LogisticRegression(C=100, max_iter=10000))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "pipe.fit(X_train, y_train)\n",
    "predict = (pipe.decision_function(X_test))\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, pipe.decision_function(X_test))\n",
    "\n",
    "plt.plot(fpr, tpr, label='LogReg')\n",
    "close_zero = np.argmin(np.abs(thresholds))\n",
    "plt.plot(fpr[close_zero], tpr[close_zero], '^', c='k',\n",
    "         markersize=10, label=' threshold zero', fillstyle='none', mew=2)\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.legend(loc='best')\n",
    "plt.title('ROC curve for logistic regression (StandardScaler(), C=100)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Use curves to determine if precision and FPR can be improved   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Generate confusion matrix with best parameters for either average preicsion or ROC AUC (these tend to have the same best performing parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision/sens/spec/fpr:\n",
    "\n",
    "precision = PPV = TP/TP+FP   \n",
    "sensitivity = recall = TPR = TP/TP+FN   \n",
    "specificity = TNR = TN/TN+FP   \n",
    "FPR = FP/FP+TN = (1-specificity)\n",
    "\n",
    "#### Errors:\n",
    "\n",
    "very major error is a susceptible prediction when isolate is resistant:    \n",
    "Very major error = (no. very major errors/no. actaul resistant)*100\n",
    "\n",
    "major error is a resitant prediction when isoalte is susceptible   \n",
    "major error = (no major erors/no. actual susceptible)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix (StandardScaler, C=100)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#build and fit pipieline\n",
    "pipe = Pipeline([('preprocessing', RobustScaler()), \n",
    "                 ('classifier', LogisticRegression(C=100, max_iter=10000))])\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "pipe.fit(X_train, y_train)\n",
    "predict = pipe.predict(X_test).astype(int)\n",
    "\n",
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predict)\n",
    "\n",
    "#calculate precision, sensitivity, specifcity, FPR, erros\n",
    "Precision = (confusion[1][1])/(confusion[1][1]+confusion[0][1])\n",
    "Sensitivity = (confusion[1][1])/(confusion[1][1]+confusion[1][0])\n",
    "Specificity = (confusion[0][0])/(confusion[0][0]+confusion[0][1])\n",
    "FPR = 1-Specificity\n",
    "very_major_error = (confusion[0][1]/y_test[y_test==0].count())*100\n",
    "major_error = (confusion[1][0]/y_test[y_test==1].count())*100\n",
    "\n",
    "\n",
    "print ('Precision: ', Precision)\n",
    "print ('Sensitivity: ', Sensitivity)\n",
    "print ('Specificity: ', Specificity)\n",
    "print ('FPR :', FPR)\n",
    "print ('very major error :', very_major_error)\n",
    "print ('major error: ', major_error)\n",
    "print ('\\n confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Shift decision function thresholds to increase specificity and precision, and reduce very major error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to change threshold, should normally do so on a validation set, not test set\n",
    "#but this is final step of tuning - therefore leakage doesnt matter, so just use test set\n",
    "\n",
    "#predict X_test with shifted threshold to 0.8 (tried other thresholds, but this seems the most reasonable)\n",
    "predict = (pipe.decision_function(X_test)>=0.8).astype(int)\n",
    "confusion = confusion_matrix(y_test, predict)\n",
    "\n",
    "Precision = (confusion[1][1])/(confusion[1][1]+confusion[0][1])\n",
    "Sensitivity = (confusion[1][1])/(confusion[1][1]+confusion[1][0])\n",
    "Specificity = (confusion[0][0])/(confusion[0][0]+confusion[0][1])\n",
    "FPR = 1-Specificity\n",
    "very_major_error = (confusion[0][1]/y_test[y_test==0].count())*100\n",
    "major_error = (confusion[1][0]/y_test[y_test==1].count())*100\n",
    "\n",
    "print ('Precision: ', Precision)\n",
    "print ('Sensitivity: ', Sensitivity)\n",
    "print ('Specificity: ', Specificity)\n",
    "print ('FPR :', FPR)\n",
    "print ('very major error :', very_major_error)\n",
    "print ('major error: ', major_error)\n",
    "\n",
    "print ('\\n confusion_matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style({'font.family':'sans-serif', 'font.sans-serif':'Helvetica'})\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                confusion.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     confusion.flatten()/np.sum(confusion)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize = (6.5,5))\n",
    "sns.heatmap(confusion, annot=labels, fmt='', cmap='Blues')\n",
    "plt.savefig('LR_binary_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indirect binary classification via multiclass MIC classification with compressed MIC labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Strategy for finding best performing model:\n",
    "1) Grid search for best performing preprocessing and parameters for *ACCURACY*   \n",
    "2) Grid search for best performing preprocessing and parameters for *weighted precision* (cannot use average_precision for multiclass classification)\n",
    "3) Generate multiclass confusion matrix and classification report using best accuracy estimator   \n",
    "4) Convert predicted test MIC to binary phenotype and resplit data with same random state for binary y_test      \n",
    "5) Generate confusion matrix and binary classification report with best parameters for accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compresss MIC labels via the following dictionary\n",
    "\n",
    "Dict = {'>=32':['>32','32.0'], '16':['16.0'], '8':['>8','8.0'], '4':['4.0'], '2':['2.0'], '1':['1.0'], \n",
    "        '0.5':['0.5'], '<=0.25':['0.25','<=0.25','<=0.06']}\n",
    "List = []\n",
    "for i in df.index:\n",
    "     for k,v in Dict.items():\n",
    "            for j in v:\n",
    "                if df['MIC'][i]==j:\n",
    "                    List.append(k)\n",
    "                    \n",
    "#add compressed labels to df (not data array)\n",
    "df['MIC_compressed'] = List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Grid search for best performing preprocessing and parameters for *ACCURACY* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build pipeline and paratmeter grid\n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', LogisticRegression(max_iter=100000000))])\n",
    "param_grid = {'preprocessing':[StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "              'classifier__C': [0.01, 0.1, 1, 10, 100]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold cross validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid, cv=Kfold_shuffle, n_jobs=-1)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set average accuracy: ', \n",
    "       accuracy_score(y_test, grid_kfold_shuffle.predict(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Grid search for best performing preprocessing and parameters for *Weighted Precision* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import make_scorer\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "#build pipeline and paratmeter grid\n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', LogisticRegression(max_iter=10000000))])\n",
    "param_grid = {'preprocessing':[StandardScaler(), MinMaxScaler(), RobustScaler(), None],\n",
    "              'classifier__C': [0.01, 0.1, 1, 10, 100]}\n",
    "scorer = make_scorer(precision_score, average = 'weighted')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Grid search with shuffled kfold cross validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid=param_grid, scoring=scorer, cv=Kfold_shuffle)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set precision score: ', \n",
    "       precision_score(y_test, grid_kfold_shuffle.predict(X_test), average='weighted', zero_division=True), '\\n')\n",
    "warnings.filterwarnings('always')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Grid search for best performing preprocessing and parameters for *weighted recall*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "#build pipeline and paratmeter grid\n",
    "#data has to be scaled for convergence\n",
    "pipe = Pipeline([('preprocessing', StandardScaler()), ('classifier', LogisticRegression(max_iter=10000000))])\n",
    "param_grid = {'preprocessing':[StandardScaler(), MinMaxScaler(), RobustScaler()],\n",
    "              'classifier__C': [0.01, 0.1, 1, 10, 100]}\n",
    "scorer = make_scorer(recall_score, average = 'weighted')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold cross validation\n",
    "\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(pipe, param_grid=param_grid, scoring=scorer, cv=Kfold_shuffle)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set recall score: ', \n",
    "       recall_score(y_test, grid_kfold_shuffle.predict(X_test), average='weighted', zero_division=True), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generate multiclass confusion matrix and classificaiton report using best precision estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#build pipeline and fit\n",
    "pipe = Pipeline([('preprocessing', None), \n",
    "                 ('classifier', LogisticRegression(C=1, max_iter=100000))])\n",
    "\n",
    "compressed_int_dict, compressed_int = {'>=32':'32', '<=0.25':'0.25'}, []\n",
    "for i in df['MIC_compressed']:\n",
    "    if i in compressed_int_dict.keys():\n",
    "        for k,v in compressed_int_dict.items():\n",
    "            if k == i:\n",
    "                compressed_int.append(v)\n",
    "    else:\n",
    "        compressed_int.append(i)\n",
    "\n",
    "            \n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, compressed_int, random_state=0)\n",
    "pipe.fit(X_train, y_train)                 \n",
    "                 \n",
    "predict = pipe.predict(X_test)\n",
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predict, labels=['0.25','0.5','1','2','4','8','16','32'])\n",
    "\n",
    "print ('\\n multiclass confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate heatmap of confusion matrix for visualisation\n",
    "\n",
    "import mglearn\n",
    "\n",
    "#this order of the target names is crucial\n",
    "target_names = ['≤0.25', '0.5', '1', '2', '4', '8', '16', '≥32']\n",
    "scores_image = mglearn.tools.heatmap(confusion, xlabel='Predicted Label',\n",
    "                                     ylabel='True Label', xticklabels=target_names,\n",
    "                                     yticklabels=target_names, cmap=plt.cm.gray_r, fmt='%d')\n",
    "plt.title('confusion matrix heat map')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('LR_multi_expand_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, predict, zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Convert predicted test MIC to binary phenotype and resplit data with same random state for binary y_test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIC_to_binary(Predict):\n",
    "    RS_dict = {1:['0.25', '0.5', '1', '2'],\n",
    "           0:['4', '8', '16', '32']}\n",
    "    binary_list = []\n",
    "    for i in predict:\n",
    "        for k,v in RS_dict.items():\n",
    "            for j in v:\n",
    "                if i == j:\n",
    "                    binary_list.append(k)\n",
    "\n",
    "    binary_array = np.array(binary_list)\n",
    "    return binary_array\n",
    "\n",
    "#convert MIC targets to binary targets\n",
    "MIC_to_binary(predict)\n",
    "\n",
    "#resplit data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'],\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generate confusion matrix and classification report with best parameters for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert MIC targets to binary targets and generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, MIC_to_binary(predict))\n",
    "\n",
    "#calculate precision, senstivity, specificty, FPR, and erros\n",
    "Precision = (confusion[1][1])/(confusion[1][1]+confusion[0][1])\n",
    "Sensitivity = (confusion[1][1])/(confusion[1][1]+confusion[1][0])\n",
    "Specificity = (confusion[0][0])/(confusion[0][0]+confusion[0][1])\n",
    "FPR = 1-Specificity\n",
    "very_major_error = (confusion[0][1]/y_test[y_test==0].count())*100\n",
    "major_error = (confusion[1][0]/y_test[y_test==1].count())*100\n",
    "\n",
    "\n",
    "print ('Precision: ', Precision)\n",
    "print ('Sensitivity: ', Sensitivity)\n",
    "print ('Specificity: ', Specificity)\n",
    "print ('FPR :', FPR)\n",
    "print ('very major error :', very_major_error)\n",
    "print ('major error: ', major_error)\n",
    "print ('\\n confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                confusion.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     confusion.flatten()/np.sum(confusion)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize = (6.5,5))\n",
    "sns.heatmap(confusion, annot=labels, fmt='', cmap='Blues')\n",
    "plt.savefig('LR_multi_binary_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
