{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for testing and optimising single tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, '../notebooks')\n",
    "\n",
    "from Build_ML_df import *\n",
    "#generate dataframe for ML\n",
    "ML_df = df_for_ML().merged_structural()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ML_df[['BINARY_PHENOTYPE', 'MIC', 'Ligand0_Distance', 'Ca_Distance', \n",
    "            'cardiolipin_Distance', 'Depth', 'lipid_head_dis', 'lipid_tail_dis',\n",
    "            'dG_stability', 'd_volume', 'd_MW', 'd_hydropathy', 'Pi', 'MAPP', \n",
    "            'H', 'B', 'E', 'G', 'I', 'T', 'S', 'NaN']].copy()\n",
    "#convert foldx stability values to floats\n",
    "df['dG_stability'] = df['dG_stability'].astype(float, errors='raise')\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#convert intermediate phenotypes to resistant\n",
    "List = []\n",
    "for i in df['BINARY_PHENOTYPE']:\n",
    "    if i == 'I':\n",
    "        List.append('R')\n",
    "    else:\n",
    "        List.append(i)\n",
    "        \n",
    "df['BINARY_PHENOTYPE'] = List\n",
    "\n",
    "#create numpy array with features for ML training\n",
    "data_array = df[df.columns[2:]].to_numpy()\n",
    "\n",
    "#create column with 01 binary phenotype\n",
    "List = []\n",
    "for i in df['BINARY_PHENOTYPE']:\n",
    "    if i == 'R':\n",
    "        List.append(0)\n",
    "    else:\n",
    "        List.append(1)\n",
    "df['BF'] = List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Direct binary classification\n",
    "\n",
    "#### Strategy for finding best performing model:\n",
    "1) Grid search for best performing preprocessing and parameters for *ACCURACY*    \n",
    "2) Plot feature importance chart\n",
    "3) Grid search for best performing preprocessing and parameters for *average precision*\n",
    "4) Grid search for best performing preprocessing and parameters for *ROC AUC\n",
    "5) enerate precision-recall curve with best parameters for *average precision*   \n",
    "6) enerate ROC curve with best parameters for *ROC AUC*   \n",
    "7) Generate confusion matrix with best parameters for either average preicsion or ROC AUC (these tend to have the same best performing parameters)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Grid search for best performing preprocessing and parameters for ACCURACY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#no preprocessing - therefore, do not see why I would need a pipeline\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, roc_auc_score\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {'max_depth':[2,4,6,8,10,12,14, None], \n",
    "              'min_samples_split':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "              'min_samples_leaf':[0.1,0.2,0.3,0.4,0.5],\n",
    "              'max_features':['auto','sqrt','log2', None]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(tree, param_grid, cv=Kfold_shuffle)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set average accuracy: ', \n",
    "       accuracy_score(y_test, grid_kfold_shuffle.predict(X_test)), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Generate feature importance plots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#define trees with best perfoming parameters, or default parameters\n",
    "tree_best_params = DecisionTreeClassifier(max_depth=2, max_features='auto', min_samples_leaf=0.1,\n",
    "                       min_samples_split=0.1, random_state=0)\n",
    "tree_no_params = DecisionTreeClassifier(random_state=0)\n",
    "tree_best_params.fit(X_train, y_train)    \n",
    "tree_no_params.fit(X_train, y_train)\n",
    "\n",
    "#plot charts\n",
    "def plot_feature_importances(model):\n",
    "    n_features = data_array.shape[1]\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), df.columns[2:-1])\n",
    "    plt.xlabel('feature importance')\n",
    "    plt.ylabel('feature')\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.title('Feature importance plot')\n",
    "\n",
    "plot_feature_importances(tree_no_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(tree_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Grid search for best performing parameters for AVERAGE PRECISION SCORE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold validation\n",
    "#use predict_proba function to calculate average_precision\n",
    "kfold_shuffle = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(tree, param_grid, cv=kfold_shuffle, scoring='average_precision')\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)                  \n",
    "print ('grid_kfold_shuffle test set average precision: ', \n",
    "       average_precision_score(y_test, grid_kfold_shuffle.predict_proba(X_test)[:,1]), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Grid search for best performing preprocessing and parameters for ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold validation\n",
    "#use predict_proba function to calculate average_precision\n",
    "\n",
    "kfold_shuffle = KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(tree, param_grid, cv=kfold_shuffle, scoring='roc_auc')\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)                  \n",
    "print ('grid_kfold_shuffle test set AUC: ', \n",
    "       roc_auc_score(y_test, grid_kfold_shuffle.predict_proba(X_test)[:,1]), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Generate precision-recall curve with best parameters for average precision   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best parameters for average precision\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.1, min_samples_split=0.1,\n",
    "                       random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, tree.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(precision, recall, label='tree')\n",
    "close_default = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.plot(precision[close_default], recall[close_default], '^', c='k', \n",
    "          markersize=10, label='threshold 0.5 ', fillstyle='none', mew=2)\n",
    "plt.xlabel('precision')\n",
    "plt.ylabel('recall')\n",
    "plt.title('Precision-recall curve for decision tree')\n",
    "plt.legend(loc='best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Generate ROC curve with best parameters for ROC AUC  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use best parameters for AUC\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_test, tree.predict_proba(X_test)[:, 1])\n",
    "\n",
    "plt.plot(fpr, tpr, label='ROC curve tree')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR(recall)')\n",
    "\n",
    "close_default = np.argmin(np.abs(thresholds - 0.5))\n",
    "plt.plot(fpr[close_default], tpr[close_default], '^', markersize=10, label='threshold 0.5', \n",
    "         fillstyle='none', c='k', mew=2)\n",
    "plt.title('ROC curve for decision tree')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Generate confusion matrix with best parameters for either average preicion or ROC AUC (these tend to have the same best performing parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision/sens/spec/fpr:\n",
    "\n",
    "precision = PPV = TP/TP+FP   \n",
    "sensitivity = recall = TPR = TP/TP+FN   \n",
    "specificity = TNR = TN/TN+FP   \n",
    "FPR = FP/FP+TN = (1-specificity)\n",
    "\n",
    "#### Errors:\n",
    "\n",
    "very major error is a susceptible prediction when isolate is resistant:    \n",
    "Very major error = (no. very major errors/no. actaul resistant)*100\n",
    "\n",
    "major error is a resitant prediction when isoalte is susceptible   \n",
    "major error = (no major erors/no. actual susceptible)*100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#confusion matrix (max_features=auto, max_depth=2, min_samples_lieaf=0.1, min_samples_split = 0)   \n",
    "   \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#build and fit decision tree\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.1, min_samples_split=0.1,\n",
    "                       random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "predict = tree.predict(X_test).astype(int)\n",
    "\n",
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predict)\n",
    "\n",
    "#calculate precision, sensitivty, specificity, FPR, errors\n",
    "Precision = (confusion[1][1])/(confusion[1][1]+confusion[0][1])\n",
    "Sensitivity = (confusion[1][1])/(confusion[1][1]+confusion[1][0])\n",
    "Specificity = (confusion[0][0])/(confusion[0][0]+confusion[0][1])\n",
    "FPR = 1-Specificity\n",
    "very_major_error = (confusion[0][1]/y_test[y_test==0].count())*100\n",
    "major_error = (confusion[1][0]/y_test[y_test==1].count())*100\n",
    "\n",
    "\n",
    "print ('Precision: ', Precision)\n",
    "print ('Sensitivity: ', Sensitivity)\n",
    "print ('Specificity: ', Specificity)\n",
    "print ('FPR :', FPR)\n",
    "print ('very major error :', very_major_error)\n",
    "print ('major error: ', major_error)\n",
    "print ('\\n confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.set_style({'font.family':'sans-serif', 'font.sans-serif':'Helvetica'})\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                confusion.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     confusion.flatten()/np.sum(confusion)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize = (6.5,5))\n",
    "sns.heatmap(confusion, annot=labels, fmt='', cmap='Blues')\n",
    "plt.savefig('tree_binary_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'], random_state=0)\n",
    "tree = DecisionTreeClassifier(max_depth=4, min_samples_leaf=0.1, min_samples_split=0.1,\n",
    "                       random_state=0)\n",
    "tree.fit(X_train, y_train)\n",
    "predicted_proba = tree.predict_proba(X_test)\n",
    "predict = (predicted_proba[:,1] >= 0.95).astype('int')\n",
    "\n",
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indirect binary classification via multiclass MIC classification with compressed MIC labels\n",
    "\n",
    "#### Strategy for finding best performing model:\n",
    "1) Grid search for best performing parameters for *ACCURACY*    \n",
    "2) Grid search for best performing parameters for *weighted precision*    \n",
    "3) Grid search for best performing parameters for *weighted recall*    \n",
    "4) Generate feature importance plot     \n",
    "5) Generate confusion matrix and classification report     \n",
    "6) Convert predicted test MIC to binary phenotype and resplit data with same random state for binary y_test          \n",
    "7) Generate confusion matrix and binary classification report with best parameters for accuracy      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compresss MIC labels via the following dictionary\n",
    "\n",
    "Dict = {'>=32':['>32','32.0'], '16':['16.0'], '8':['>8','8.0'], '4':['4.0'], '2':['2.0'], '1':['1.0'], \n",
    "        '0.5':['0.5'], '<=0.25':['0.25','<=0.25','<=0.06']}\n",
    "List = []\n",
    "for i in df.index:\n",
    "     for k,v in Dict.items():\n",
    "            for j in v:\n",
    "                if df['MIC'][i]==j:\n",
    "                    List.append(k)\n",
    "                    \n",
    "#add compressed labels to df (not data array)\n",
    "df['MIC_compressed'] = List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Grid search for best performing preprocessing and parameters for *ACCURACY* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "param_grid = {'max_depth':[2,4,6,8,10,12,14, None], \n",
    "              'min_samples_split':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "              'min_samples_leaf':[0.1,0.2,0.3,0.4,0.5],\n",
    "              'max_features':['auto','sqrt','log2', None]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with kfold cross validation (best cv for logreg multiclass classification)\n",
    "Kfold=KFold(n_splits=5)\n",
    "grid_kfold = GridSearchCV(tree, param_grid, cv=Kfold)\n",
    "grid_kfold.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold.best_score_)\n",
    "print ('grid_kfold_shuffle: test set average accuracy: ', \n",
    "       accuracy_score(y_test, grid_kfold.predict(X_test)), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Grid search for best performing preprocessing and parameters for *weighted precision*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, precision_score\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "scorer = make_scorer(precision_score, average='weighted')\n",
    "param_grid =  {'max_depth':[2,4,6,8,10,12,14, None], \n",
    "              'min_samples_split':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "              'min_samples_leaf':[0.1,0.2,0.3,0.4,0.5],\n",
    "              'max_features':['auto','sqrt','log2', None]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold cross validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(tree, param_grid=param_grid, scoring=scorer, cv=Kfold_shuffle, n_jobs=-1)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set precision score: ', \n",
    "       precision_score(y_test, grid_kfold_shuffle.predict(X_test), average='weighted', zero_division=True), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Grid search for best performing preprocessing and parameters for *weighted recall*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "\n",
    "tree = DecisionTreeClassifier(random_state=0)\n",
    "scorer = make_scorer(recall_score, average='weighted')\n",
    "param_grid =  {'max_depth':[2,4,6,8,10,12,14, None], \n",
    "              'min_samples_split':[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0],\n",
    "              'min_samples_leaf':[0.1,0.2,0.3,0.4,0.5],\n",
    "              'max_features':['auto','sqrt','log2', None]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['MIC_compressed'],\n",
    "                                                    random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid search with shuffled kfold cross validation\n",
    "Kfold_shuffle=KFold(n_splits=5, shuffle=True, random_state=0)\n",
    "grid_kfold_shuffle = GridSearchCV(tree, param_grid=param_grid, scoring=scorer, cv=Kfold_shuffle, n_jobs=-1)\n",
    "grid_kfold_shuffle.fit(X_train, y_train)\n",
    "print ('grid_kfold_shuffle: best estimator: \\n', grid_kfold_shuffle.best_estimator_)\n",
    "print ('grid_kfold_shuffle: best cross-validation score: ', grid_kfold_shuffle.best_score_)\n",
    "print ('grid_kfold_shuffle: test set recall score: ', \n",
    "       recall_score(y_test, grid_kfold_shuffle.predict(X_test), average='weighted', zero_division=True), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Generate feature importance plots (best params for weighted precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define trees with best perfoming parameters, or default parameters\n",
    "tree_best_params = DecisionTreeClassifier(max_depth=4, max_features='auto', min_samples_leaf=0.1,\n",
    "                       min_samples_split=0.1, random_state=0)\n",
    "tree_no_params = DecisionTreeClassifier(random_state=0)\n",
    "tree_best_params.fit(X_train, y_train)    \n",
    "tree_no_params.fit(X_train, y_train)\n",
    "\n",
    "#plot charts\n",
    "def plot_feature_importances(model):\n",
    "    n_features = data_array.shape[1]\n",
    "    plt.barh(np.arange(n_features), model.feature_importances_, align='center')\n",
    "    plt.yticks(np.arange(n_features), df.columns[2:-2])\n",
    "    plt.xlabel('feature importance')\n",
    "    plt.ylabel('feature')\n",
    "    plt.ylim(-1, n_features)\n",
    "    plt.title('Feature importance plot')\n",
    "\n",
    "plot_feature_importances(tree_no_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_feature_importances(tree_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Generate multiclass confusion matrix and classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#copy and fit classifier with best parameters from previous section\n",
    "tree = tree_best_params\n",
    "tree.fit(X_train, y_train)                 \n",
    "                 \n",
    "predict = tree.predict(X_test)\n",
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, predict)\n",
    "\n",
    "print ('\\n confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate heatmap of confusion matrix for visualisation\n",
    "\n",
    "import mglearn\n",
    "\n",
    "#this order of the target names is crucial\n",
    "target_names = ['≤0.25', '0.5', '1', '2', '4', '8', '16', '≥32']\n",
    "scores_image = mglearn.tools.heatmap(confusion, xlabel='Predicted Label',\n",
    "                                     ylabel='True Label', xticklabels=target_names,\n",
    "                                     yticklabels=target_names, cmap=plt.cm.gray_r, fmt='%d')\n",
    "plt.title('confusion matrix heat map')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.savefig('LR_multi_expand_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print (classification_report(y_test, predict, zero_division=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Convert predicted test MIC to binary phenotype and resplit data with same random state for binary y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def MIC_to_binary(Predict):\n",
    "    RS_dict = {1:['0.25', '0.5', '1', '2'],\n",
    "           0:['4', '8', '16', '32']}\n",
    "    binary_list = []\n",
    "    for i in predict:\n",
    "        for k,v in RS_dict.items():\n",
    "            for j in v:\n",
    "                if i == j:\n",
    "                    binary_list.append(k)\n",
    "\n",
    "    binary_array = np.array(binary_list)\n",
    "    return binary_array\n",
    "\n",
    "#convert MIC targets to binary targets\n",
    "MIC_to_binary(predict)\n",
    "\n",
    "#resplit data\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_array, df['BF'],\n",
    "                                                  random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Generate confusion matrix and classification report with best parameters for accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate confusion matrix\n",
    "confusion = confusion_matrix(y_test, MIC_to_binary(predict))\n",
    "\n",
    "#calculate precision, sensitivity, specificty, FPR, and errors\n",
    "Precision = (confusion[1][1])/(confusion[1][1]+confusion[0][1])\n",
    "Sensitivity = (confusion[1][1])/(confusion[1][1]+confusion[1][0])\n",
    "Specificity = (confusion[0][0])/(confusion[0][0]+confusion[0][1])\n",
    "FPR = 1-Specificity\n",
    "very_major_error = (confusion[0][1]/y_test[y_test==0].count())*100\n",
    "major_error = (confusion[1][0]/y_test[y_test==1].count())*100\n",
    "\n",
    "\n",
    "print ('Precision: ', Precision)\n",
    "print ('Sensitivity: ', Sensitivity)\n",
    "print ('Specificity: ', Specificity)\n",
    "print ('FPR :', FPR)\n",
    "print ('very major error :', very_major_error)\n",
    "print ('major error: ', major_error)\n",
    "print ('\\n confusion matrix: \\n', confusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (classification_report(y_test, MIC_to_binary(predict)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "                confusion.flatten()]\n",
    "group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "                     confusion.flatten()/np.sum(confusion)]\n",
    "labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "          zip(group_names,group_counts,group_percentages)]\n",
    "labels = np.asarray(labels).reshape(2,2)\n",
    "plt.figure(figsize = (6.5,5))\n",
    "sns.heatmap(confusion, annot=labels, fmt='', cmap='Blues')\n",
    "plt.savefig('tree_multi_binary_cf.png', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
